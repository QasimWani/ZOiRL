{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qasimwani/opt/anaconda3/lib/python3.7/site-packages/patsy/constraint.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping\n",
      "/Users/qasimwani/Desktop/research/RL/CityLearn/ROLEVT/actor.py:919: DeprecationWarning: invalid escape sequence \\d\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"always\")\n",
    "\n",
    "# Run this again after editing submodules so Colab uses the updated versions\n",
    "from citylearn import  CityLearn\n",
    "from pathlib import Path\n",
    "from TD3 import Agent as Agent\n",
    "import numpy as np                                                                                                                                                                                      \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from utils import agent_checkpoint_cost\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to add RBC pretrained model (sec): 19.869\n"
     ]
    }
   ],
   "source": [
    "# Load environment\n",
    "climate_zone = 1\n",
    "end_time = 6000 # total number of hours to train for\n",
    "cost_analysis = 24 # analyze costs every x hours\n",
    "checkpoint = 24 # save parameters every x hours\n",
    "\n",
    "params = {'data_path':Path(\"data/Climate_Zone_\"+str(climate_zone)), \n",
    "        'building_attributes':'building_attributes.json', \n",
    "        'weather_file':'weather_data.csv', \n",
    "        'solar_profile':'solar_generation_1kW.csv', \n",
    "        'carbon_intensity':'carbon_intensity.csv',\n",
    "        'building_ids':[\"Building_\"+str(i) for i in [1,2,3,4,5,6,7,8,9]],\n",
    "        'buildings_states_actions':'buildings_state_action_space.json', \n",
    "        'simulation_period': (0, end_time), \n",
    "        'cost_function': ['ramping','1-load_factor','average_daily_peak','peak_demand','net_electricity_consumption','carbon_emissions'], \n",
    "        'central_agent': False,\n",
    "        'cost_analysis' : cost_analysis,\n",
    "        'save_memory': False }\n",
    "\n",
    "# Contain the lower and upper bounds of the states and actions, to be provided to the agent to normalize the variables between 0 and 1.\n",
    "env = CityLearn(**params)\n",
    "env.pretrain_baseline_model((0, end_time))\n",
    "\n",
    "observations_spaces, actions_spaces = env.get_state_action_spaces()\n",
    "\n",
    "# Provides information on Building type, Climate Zone, Annual DHW demand, Annual Cooling Demand, Annual Electricity Demand, Solar Capacity, and correllations among buildings\n",
    "building_info = env.get_building_information()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mse_critic():\n",
    "    if not agent.did_i_just_finish_training():\n",
    "            return None, None\n",
    "    try:\n",
    "        data = {}\n",
    "        NUM_DAYS = len(agent.critic_optim.debug['ramping_cost'])\n",
    "\n",
    "        for key, _d in agent.critic_optim.debug.items():\n",
    "            data[key] = []\n",
    "            for day in _d:\n",
    "                data[key].append(day.value)\n",
    "            data[key] = np.reshape(data[key], (NUM_DAYS * 24))\n",
    "\n",
    "        rc = data['ramping_cost']\n",
    "        peak = data['peak_net_electricity_cost']\n",
    "        ec = data['electricity_cost']\n",
    "\n",
    "        A = np.vstack((-rc, -peak)).T\n",
    "        y = np.reshape(agent.critic_optim.problem.param_dict['y_r'].value, (NUM_DAYS * 24))\n",
    "\n",
    "        x, residuals, rank, s = np.linalg.lstsq(A, y, rcond=None)\n",
    "        # print(f\"Rank: {rank}\\tCondition Number: {np.linalg.cond(A)}\")\n",
    "        # print(f\"MSE across {NUM_DAYS} days: {round(np.linalg.norm(A@x - y), 5)}\")\n",
    "        return round(np.linalg.norm(A@x - y), 5), round(np.linalg.cond(A), 3)\n",
    "    except:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iAC: 1.413 , ZOiRL: 1.406 , %ipr: -0.503:   8%|▊         | 479/6000 [00:34<09:59,  9.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E2E\tBuilding: 2, r: 01our: 23"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iAC: 1.413 , ZOiRL: 1.406 , %ipr: -0.503:   8%|▊         | 479/6000 [10:52<2:05:24,  1.36s/it]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Loss must be negative, got: 9.704579338198549",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-73e13e6af9f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/research/RL/CityLearn/ROLEVT/TD3.py\u001b[0m in \u001b[0;36madd_to_buffer\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m    279\u001b[0m         ):\n\u001b[1;32m    280\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mLOG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Time taken for training: {round(end - start, 2)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/research/RL/CityLearn/ROLEVT/TD3.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# local + target actor update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/research/RL/CityLearn/ROLEVT/TD3.py\u001b[0m in \u001b[0;36mactor_update\u001b[0;34m(self, parameters)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTEMP_VAR\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# self.buildings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;31m# local actor update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;31m# target actor update - moving average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/research/RL/CityLearn/ROLEVT/actor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, t, critic, batch_parameters, building_id)\u001b[0m\n\u001b[1;32m    934\u001b[0m                     \u001b[0meta_ehH_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mc_bat_end_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m                 ) = self.E2E_grad(r, day_param, critic, building_id)\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m                 \u001b[0;31m# store gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/research/RL/CityLearn/ROLEVT/actor.py\u001b[0m in \u001b[0;36mE2E_grad\u001b[0;34m(self, t, parameters, critic, building_id)\u001b[0m\n\u001b[1;32m    884\u001b[0m         \"\"\"\n\u001b[1;32m    885\u001b[0m         loss, dq_da = self.gradient_actions(\n\u001b[0;32m--> 886\u001b[0;31m             \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuilding_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m         )\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdq_da\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/research/RL/CityLearn/ROLEVT/actor.py\u001b[0m in \u001b[0;36mgradient_actions\u001b[0;34m(self, t, parameters, critic, building_id)\u001b[0m\n\u001b[1;32m    778\u001b[0m         assert (\n\u001b[1;32m    779\u001b[0m             \u001b[0mreward_warping_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         ), f\"Loss must be negative, got: {reward_warping_loss.item()}\"\n\u001b[0m\u001b[1;32m    781\u001b[0m         \u001b[0;31m# add virtual electricity cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Loss must be negative, got: 9.704579338198549"
     ]
    }
   ],
   "source": [
    "params_agent = {'building_ids':[\"Building_\"+str(i) for i in [1,2,3,4,5,6,7,8,9]],\n",
    "                 'buildings_states_actions':'buildings_state_action_space.json', \n",
    "                 'building_info':building_info,\n",
    "                 'observation_spaces':observations_spaces,\n",
    "                 'agent_checkpoint' : checkpoint,\n",
    "                 'action_spaces':actions_spaces}\n",
    "\n",
    "# Instantiating the control agent(s)\n",
    "rewards = []\n",
    "critic_mse = [] # only last building to be added MSE will be recorded\n",
    "\n",
    "agent = Agent(**params_agent)\n",
    "\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "ZO_iRL_costs = np.load(\"ZOiRL_cost_curve.npy\")\n",
    "\n",
    "with tqdm(total=end_time) as pbar:\n",
    "    while not done:\n",
    "        \n",
    "        # get agent\n",
    "        # day_type = env.buildings['Building_1'].sim_results['day'][env.time_step]\n",
    "        # agent = agents.get_agent(day_type)\n",
    "        # get agent\n",
    "        \n",
    "        action, _ = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.add_to_buffer(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "        try:\n",
    "            mse, condition = get_mse_critic()\n",
    "            pim = round(100 * (ZO_iRL_costs[env.time_step] - env.costs_periodic[-1]['total']) / ZO_iRL_costs[env.time_step], 3)\n",
    "            txt = f\"iAC: {round(env.costs_periodic[-1]['total'], 3)} , ZOiRL: {round(ZO_iRL_costs[env.time_step], 3)} , %ipr: {pim}\"\n",
    "            if len(critic_mse) > 0:\n",
    "                txt += f\", MSE: {round(critic_mse[-1][0], 3)} , CN: {round(critic_mse[-1][1], 3)}\"\n",
    "                \n",
    "            if mse is not None:\n",
    "                critic_mse.append([mse, condition])\n",
    "                \n",
    "            pbar.set_description(txt)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        pbar.update(1)\n",
    "\n",
    "rewards = np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.critic[0].alpha_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=23\n",
    "agent.critic[0].prob[i].solve('SCS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.critic[0].prob[i].solve('SCS', eps=5e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = agent.critic[0].prob[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-r * ramping_cost - electricity_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.solve(solver='SCS', max_iters=10_000_000, eps=5e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(e[t:] * E_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_type = 0\n",
    "Q_value, ramping_cost, peak_hist_cost, electricity_cost, E_grid, E_grid_true, E_grid_prevhour, (bid, t) = agent.critic_target[_type].debug\n",
    "r, e = agent.critic[_type].alpha_ramp[bid], agent.critic[_type].alpha_elec[bid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(agent.actor._grads[0]['p_ele_grad'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.array(critic_mse)[:, 0])\n",
    "plt.xlabel(\"Meta-episode #\")\n",
    "plt.ylabel(\"Cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor.debug[1][\"E_grid_prevhour\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(rewards)[23:, 0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(critic_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in agent.actor._losses.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(rewards).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = [x[\"total\"] for x in env.costs_periodic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(agent.actor._grads[0]['p_ele_grad'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.cost_detailed_view()['Building_9']['cost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.cost((0, end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total, label=\"iAC\")\n",
    "plt.plot(ZO_iRL_costs[::24][1:], label=\"ZOiRL\")\n",
    "plt.legend()\n",
    "plt.ylim(0.96, 1.5)\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(ZO_iRL_costs[::24][1:] < total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-66989084.0,rewards[23:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "NUM_DAYS = len(agent.critic_optim.debug['ramping_cost'])\n",
    "\n",
    "for key, _d in agent.critic_optim.debug.items():\n",
    "    data[key] = []\n",
    "    for day in _d:\n",
    "        data[key].append(day.value)\n",
    "    data[key] = np.reshape(data[key], (NUM_DAYS * 24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = data['ramping_cost']\n",
    "peak = data['peak_net_electricity_cost']\n",
    "ec = data['electricity_cost']\n",
    "\n",
    "A = np.vstack((-rc, -peak)).T\n",
    "y = np.reshape(agent.critic_optim.problem.param_dict['y_r'].value, (NUM_DAYS * 24))\n",
    "\n",
    "x, residuals, rank, s = np.linalg.lstsq(A, y, rcond=None)\n",
    "print(f\"Rank: {rank}\\tCondition Number: {np.linalg.cond(A)}\")\n",
    "print(f\"MSE across {NUM_DAYS} days: {round(np.linalg.norm(A@x - y), 5)}\")\n",
    "\n",
    "plt.plot(A@x, label='y_hat')\n",
    "plt.plot(y, label='y')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Hours\")\n",
    "plt.ylabel(\"Q-value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(A@x).max(), y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = agent.critic_optim.problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy\n",
    "for i in range(len(p.constraints)):\n",
    "    x = p.constraints[i]\n",
    "    if isinstance(x.args[0], cvxpy.atoms.affine.add_expr.AddExpression):\n",
    "        assert x.args[0].value <= 1, (i, x.args[0].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.critic[0].alpha_peak1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.critic_optim.debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(agent.critic[0].alpha_elec, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.critic_target[0].alpha_ramp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.critic_target[0].alpha_bias[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_t = np.array(agent._actor_zetas['p_ele_local']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(24), agent.actor.zeta['p_ele'][:, 0], c='k', label=\"Iteration 3\")\n",
    "plt.bar(range(24), _t[0, :, 0], label=\"Initial\")\n",
    "plt.bar(range(24), _t[0, :, 1], label=\"Iteration 1\")\n",
    "plt.bar(range(24), _t[0, :, 2], label=\"Iteration 2\")\n",
    "plt.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 1.15), ncol=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seaborn plotting aesthetics as default\n",
    "sns.set(rc={'figure.figsize':(5,5)})\n",
    "df = np.array(agent._actor_zetas['p_ele_local'])\n",
    "df = np.append(df, np.expand_dims(agent.actor.zeta['p_ele'], 0)).reshape((9, 24, 9)).T\n",
    "\n",
    "fig, axn = plt.subplots(2, 2, sharex=True, sharey=True)\n",
    "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
    "\n",
    "for i, ax in enumerate(axn.flat):\n",
    "    sns.heatmap(df[i], ax=ax,\n",
    "                cbar=i == 0,\n",
    "                cmap='viridis',\n",
    "                cbar_ax=None if i else cbar_ax)\n",
    "    ax.set(title=f\"Building {i + 1}\")\n",
    "\n",
    "axn.flat[0].set(ylabel='Hour of Day')\n",
    "axn.flat[2].set(ylabel='Hour of Day')\n",
    "axn.flat[2].set(xlabel='Month')\n",
    "axn.flat[3].set(xlabel='Month')\n",
    "\n",
    "\n",
    "fig.tight_layout(rect=[0, 0, .9, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(24), agent.actor.zeta['p_ele'][:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(agent._actor_zetas[\"p_ele_local\"])[:, :, 0].min(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(agent._actor_zetas[\"p_ele_target\"])[:, :, 0].min(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor.scs_cnt, agent.actor.fail_cnt, agent.critic_optim.fail_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor.zeta[\"p_ele\"][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.critic[0].alpha_peak1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.critic_target[0].alpha_peak1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor.zeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.random.rand(101, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.expand_dims(t, axis=(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total[100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
